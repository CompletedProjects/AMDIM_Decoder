{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pixel_CNNpp_V2_150220_Wandb.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNdSBuiifdyumruBuxU1teC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nerdk312/AMDIM_Decoder/blob/master/Pixel_CNNpp_V2_150220_Wandb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUAWFOsoc6Lf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "function ClickConnect(){\n",
        "console.log(\"Working\"); \n",
        "document.querySelector(\"colab-toolbar-button#connect\").click() \n",
        "}\n",
        "setInterval(ClickConnect,60000)\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AuxsM37XGGUW",
        "colab_type": "code",
        "outputId": "75e7de7a-21eb-4bc7-dd43-af3538756669",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install wandb"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wandb\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/62/7c/cd5ef2bc3aa0597b5fd5645357f0207c02345ec2f3569dcbb56e6dafa070/wandb-0.8.27-py2.py3-none-any.whl (1.4MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4MB 2.9MB/s \n",
            "\u001b[?25hCollecting configparser>=3.8.1\n",
            "  Downloading https://files.pythonhosted.org/packages/7a/2a/95ed0501cf5d8709490b1d3a3f9b5cf340da6c433f896bbe9ce08dbe6785/configparser-4.0.2-py2.py3-none-any.whl\n",
            "Collecting sentry-sdk>=0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/23/5a/f1b0c63e40517b06bc21744a94013ca05de21de2687a59de889ea20a9ebd/sentry_sdk-0.14.1-py2.py3-none-any.whl (93kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 10.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (5.4.8)\n",
            "Collecting subprocess32>=3.5.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/c8/564be4d12629b912ea431f1a50eb8b3b9d00f1a0b1ceff17f266be190007/subprocess32-3.5.4.tar.gz (97kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 9.3MB/s \n",
            "\u001b[?25hCollecting gql==0.2.0\n",
            "  Downloading https://files.pythonhosted.org/packages/c4/6f/cf9a3056045518f06184e804bae89390eb706168349daa9dff8ac609962a/gql-0.2.0.tar.gz\n",
            "Requirement already satisfied: Click>=7.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (7.0)\n",
            "Requirement already satisfied: nvidia-ml-py3>=7.352.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (7.352.0)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/80/d7/2bfc9332e68d3e15ea97b9b1588b3899ad565120253d3fd71c8f7f13b4fe/shortuuid-0.5.0.tar.gz\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (2.21.0)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f5/e8/f6bd1eee09314e7e6dee49cbe2c5e22314ccdb38db16c9fc72d2fa80d054/docker_pycreds-0.4.0-py2.py3-none-any.whl\n",
            "Collecting GitPython>=1.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/d2/5b44bbfea118bd97d94549b8a0eb2fd60e619c8fdeead4fa7cf666738cbd/GitPython-3.0.7-py3-none-any.whl (451kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 20.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (1.12.0)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from wandb) (2.6.1)\n",
            "Requirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.6/dist-packages (from wandb) (3.13)\n",
            "Collecting watchdog>=0.8.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/73/c3/ed6d992006837e011baca89476a4bbffb0a91602432f73bd4473816c76e2/watchdog-0.10.2.tar.gz (95kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 9.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from sentry-sdk>=0.4.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from sentry-sdk>=0.4.0->wandb) (2019.11.28)\n",
            "Collecting graphql-core<2,>=0.5.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b0/89/00ad5e07524d8c523b14d70c685e0299a8b0de6d0727e368c41b89b7ed0b/graphql-core-1.1.tar.gz (70kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 8.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.6/dist-packages (from gql==0.2.0->wandb) (2.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->wandb) (2.8)\n",
            "Collecting gitdb2>=2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/17/5e/4ec98f192bbf783917c0d68f23fb588a8f394fed67b90197786bdb30d0c7/gitdb2-3.0.0-py2.py3-none-any.whl (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 8.8MB/s \n",
            "\u001b[?25hCollecting pathtools>=0.1.1\n",
            "  Downloading https://files.pythonhosted.org/packages/e7/7f/470d6fcdf23f9f3518f6b0b76be9df16dcc8630ad409947f8be2eb0ed13a/pathtools-0.1.2.tar.gz\n",
            "Collecting smmap2>=2.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/55/d2/866d45e3a121ee15a1dc013824d58072fd5c7799c9c34d01378eb262ca8f/smmap2-2.0.5-py2.py3-none-any.whl\n",
            "Building wheels for collected packages: subprocess32, gql, shortuuid, watchdog, graphql-core, pathtools\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for subprocess32: filename=subprocess32-3.5.4-cp36-none-any.whl size=6489 sha256=f62b84a72cc07fb82169012771a24f4c694fdb8bfe2b5e99de7fd86f07052086\n",
            "  Stored in directory: /root/.cache/pip/wheels/68/39/1a/5e402bdfdf004af1786c8b853fd92f8c4a04f22aad179654d1\n",
            "  Building wheel for gql (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gql: filename=gql-0.2.0-cp36-none-any.whl size=7630 sha256=d019553807506bf3f0ae63547e655bd8c2dbb4a854e1974f86d0409b053f0721\n",
            "  Stored in directory: /root/.cache/pip/wheels/ce/0e/7b/58a8a5268655b3ad74feef5aa97946f0addafb3cbb6bd2da23\n",
            "  Building wheel for shortuuid (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for shortuuid: filename=shortuuid-0.5.0-cp36-none-any.whl size=5497 sha256=f4deb0177521dd38904b3f1fd3a821d2fffdb53b4baa655a0bffd0767fc578bf\n",
            "  Stored in directory: /root/.cache/pip/wheels/3f/eb/fd/69e5177f67b505e44acbd1aedfbe44b91768ee0c4cd5636576\n",
            "  Building wheel for watchdog (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for watchdog: filename=watchdog-0.10.2-cp36-none-any.whl size=73605 sha256=33b286fe81ee9370b02521ceed75334ed151d55c0622d28d2f3c3f80891e8ece\n",
            "  Stored in directory: /root/.cache/pip/wheels/bc/ed/6c/028dea90d31b359cd2a7c8b0da4db80e41d24a59614154072e\n",
            "  Building wheel for graphql-core (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for graphql-core: filename=graphql_core-1.1-cp36-none-any.whl size=104650 sha256=db33aec28a14cb8081239f437824745865d62e1959ddcdd6a687afd4959ab974\n",
            "  Stored in directory: /root/.cache/pip/wheels/45/99/d7/c424029bb0fe910c63b68dbf2aa20d3283d023042521bcd7d5\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-cp36-none-any.whl size=8784 sha256=5855eeb8861382fb3d1204f61d1b24eb4cd72b65b2625b815d7415fcb5911b0e\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/04/79/c3b0c3a0266a3cb4376da31e5bfe8bba0c489246968a68e843\n",
            "Successfully built subprocess32 gql shortuuid watchdog graphql-core pathtools\n",
            "Installing collected packages: configparser, sentry-sdk, subprocess32, graphql-core, gql, shortuuid, docker-pycreds, smmap2, gitdb2, GitPython, pathtools, watchdog, wandb\n",
            "Successfully installed GitPython-3.0.7 configparser-4.0.2 docker-pycreds-0.4.0 gitdb2-3.0.0 gql-0.2.0 graphql-core-1.1 pathtools-0.1.2 sentry-sdk-0.14.1 shortuuid-0.5.0 smmap2-2.0.5 subprocess32-3.5.4 wandb-0.8.27 watchdog-0.10.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97XHLoEbGTsn",
        "colab_type": "code",
        "outputId": "80c29b93-aaa0-4f3e-b242-07d170b422df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "!wandb login <API KEY>"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[32mSuccessfully logged in to Weights & Biases!\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mG1nsgvv_rwe",
        "colab_type": "code",
        "outputId": "376ba8db-2864-46b4-b373-aae331482086",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        }
      },
      "source": [
        "from google.colab import drive\n",
        "#drive.flush_and_unmount()\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ek719BAlAR0A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "from __future__ import print_function\n",
        "import pickle\n",
        "import sys\n",
        "#sys.path.append('/content/gdrive/My Drive/pixel-cnn-pp_V2_090220')\n",
        "\n",
        "sys.path.append('/content/gdrive/My Drive/pixel-cnn-pp_V2_110220_Colab')\n",
        "\n",
        "import wandb\n",
        "\n",
        "import os\n",
        "import argparse\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from torch.optim import lr_scheduler\n",
        "from torchvision import datasets, transforms, utils\n",
        "#from tensorboardX import SummaryWriter\n",
        "from utils import * \n",
        "from model import * \n",
        "from PIL import Image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSeau2tbvAaX",
        "colab_type": "text"
      },
      "source": [
        "# Function to sample an image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5Y1UIgBuqal",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sample(model):\n",
        "    model.train(False)\n",
        "    data = torch.zeros(sample_batch_size, obs[0], obs[1], obs[2])\n",
        "    data = data.cuda()\n",
        "    for i in range(obs[1]):\n",
        "        for j in range(obs[2]):\n",
        "            data_v = Variable(data, volatile=True)\n",
        "            out,_,_ = model(data_v, sample=True) # Nawid - Add to add 2 empty outputs as I do not require the log and the variance I believe\n",
        "            out_sample = sample_op(out)\n",
        "            data[:, :, i, j] = out_sample.data[:, :, i, j]\n",
        "    return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHFif3sasbaz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def reconstruct_image(model,data_loader):\n",
        "    input, labels = iter(data_loader).next()\n",
        "    model.train(False)\n",
        "    input = input.cuda(async=True)\n",
        "    input_var = Variable(input)\n",
        "    out, _, _ = model(input_var) \n",
        "    out_sample =sample_op(out)\n",
        "    return out_sample, input"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d42tvIkgvvaj",
        "colab_type": "text"
      },
      "source": [
        "# Model Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xi6eBAibu-2X",
        "colab_type": "code",
        "outputId": "6a28be7f-4ecb-4e96-bc35-53f80bf90349",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "bc4 = False\n",
        "args = {'data_dir':'data' if bc4 else '/content/gdrive/My Drive/pixel_CNN_data',\n",
        "        'save_dir':'models',\n",
        "        'dataset':'cifar',\n",
        "        'print_every':50,\n",
        "        'save_interval': 3,\n",
        "        'load_params':None,\n",
        "        'nr_resnet': 3,\n",
        "        'nr_filters': 60,\n",
        "        'nr_logistic_mix':10,\n",
        "        'lr':0.0002,\n",
        "        'lr_decay':0.999995,\n",
        "        'batch_size':16,\n",
        "        'max_epochs':100,\n",
        "        'seed':1\n",
        "}\n",
        "\n",
        "if bc4: # Nawid-  Uses argparser if using bluecrystal\n",
        "    parser = argparse.ArgumentParser()\n",
        "    # data I/O\n",
        "    parser.add_argument('-i', '--data_dir', type=str,\n",
        "    default='data', help='Location for the dataset')\n",
        "    parser.add_argument('-o', '--save_dir', type=str, default='models',\n",
        "    help='Location for parameter checkpoints and samples')\n",
        "\n",
        "    parser.add_argument('-d', '--dataset', type=str,\n",
        "    default='cifar', help='Can be either cifar|mnist')\n",
        "    parser.add_argument('-p', '--print_every', type=int, default=50,\n",
        "    help='how many iterations between print statements')\n",
        "    parser.add_argument('-t', '--save_interval', type=int, default=3,\n",
        "    help='Every how many epochs to write checkpoint/samples?')\n",
        "    parser.add_argument('-r', '--load_params', type=str, default=None,\n",
        "    help='Restore training from previous model checkpoint?')\n",
        "    # model\n",
        "    parser.add_argument('-q', '--nr_resnet', type=int, default=3,\n",
        "    help='Number of residual blocks per stage of the model')\n",
        "    parser.add_argument('-n', '--nr_filters', type=int, default=60,\n",
        "    help='Number of filters to use across the model. Higher = larger model.')\n",
        "    parser.add_argument('-m', '--nr_logistic_mix', type=int, default=10,\n",
        "    help='Number of logistic components in the mixture. Higher = more flexible model')\n",
        "    parser.add_argument('-l', '--lr', type=float,\n",
        "    default=0.0002, help='Base learning rate')\n",
        "    parser.add_argument('-e', '--lr_decay', type=float, default=0.999995,\n",
        "    help='Learning rate decay, applied every step of the optimization')\n",
        "    parser.add_argument('-b', '--batch_size', type=int, default=32,\n",
        "    help='Batch size during training per GPU')\n",
        "    parser.add_argument('-x', '--max_epochs', type=int,\n",
        "    default=100, help='How many epochs to run in total?')\n",
        "    parser.add_argument('-s', '--seed', type=int, default=1,\n",
        "    help='Random seed to use')\n",
        "    command_args = parser.parse_args()\n",
        "\n",
        "    args['data_dir'] = command_args.data_dir\n",
        "    args['save_dir'] = command_args.save_dir\n",
        "    args['dataset'] = command_args.dataset\n",
        "    args['print_every'] = command_args.print_every\n",
        "    args['save_interval'] = command_args.save_interval\n",
        "    args['load_params'] = command_args.load_params\n",
        "    args['nr_resnet'] = command_args.nr_resnet\n",
        "    args['nr_filters'] = command_args.nr_filters\n",
        "    args['nr_logistic_mix'] = command_args.nr_logistic_mix\n",
        "    args['lr'] = command_args.lr\n",
        "    args['lr_decay'] = command_args.lr_decay\n",
        "    args['batch_size'] = command_args.batch_size\n",
        "    args['max_epochs'] = command_args.max_epochs\n",
        "    args['seed'] = command_args.seed\n",
        "\n",
        "print(args['data_dir'])\n",
        "wandb.init(entity=\"nerdk312\", project=\"pytorch-PixelCNN\",config=args)\n",
        "\n",
        "torch.manual_seed(args['seed'])\n",
        "np.random.seed(args['seed'])\n",
        "\n",
        "model_name = 'pcnn_lr:{:.5f}_nr-resnet{}_nr-filters{}'.format(args['lr'], args['nr_resnet'], args['nr_filters'])\n",
        "\n",
        "#model_folder = os.path.join('/content/gdrive/My Drive/pixel_cnn_runs', model_name)\n",
        "#assert not os.path.exists(os.path.join('/content/gdrive/My Drive/pixel_cnn_runs', model_name)), '{} already exists!'.format(model_name)\n",
        "#writer = SummaryWriter(log_dir= model_folder) # Nawid - Changed the name where the information is saved\n",
        "\n",
        "#os.mkdir(model_folder) # Nawid - Used to make the folder when the summary writer is present\n",
        "\n",
        "#epoch_models = os.path.join(model_folder,'models')\n",
        "#images_models = os.path.join(model_folder,'images') \n",
        "\n",
        "#os.mkdir(epoch_models)\n",
        "#os.mkdir(images_models)\n",
        "\n",
        "sample_batch_size = 25\n",
        "obs = (3, 64, 64) if 'STL10' in args['dataset'] else (3, 32, 32)\n",
        "\n",
        "input_channels = obs[0]\n",
        "desired_size = obs[1]\n",
        "\n",
        "rescaling     = lambda x : (x - .5) * 2.\n",
        "rescaling_inv = lambda x : .5 * x  + .5\n",
        "kwargs = {'num_workers':1, 'pin_memory':True, 'drop_last':True}\n",
        "\n",
        "if 'STL10' in args['dataset']:\n",
        "    ds_transforms = transforms.Compose([transforms.CenterCrop(desired_size),transforms.ToTensor(), rescaling]) \n",
        "  \n",
        "else:\n",
        "    ds_transforms = transforms.Compose([transforms.ToTensor(), rescaling])\n",
        "\n",
        "image_dim = desired_size\n",
        "\n",
        "\n",
        "if 'cifar' in args['dataset']:\n",
        "    train_loader = torch.utils.data.DataLoader(datasets.CIFAR10(args['data_dir'], train=True, \n",
        "                                                              download=True, transform=ds_transforms), batch_size=args['batch_size'], shuffle=True, **kwargs)\n",
        "    test_loader  = torch.utils.data.DataLoader(datasets.CIFAR10(args['data_dir'], train=False, \n",
        "                                                              transform=ds_transforms), batch_size=args['batch_size'], shuffle=True, **kwargs)\n",
        "    loss_op   = lambda real, fake : discretized_mix_logistic_loss(real, fake)\n",
        "    elbo_op = lambda mean, logvariance: elbo_loss(mean,logvariance)\n",
        "    sample_op = lambda x : sample_from_discretized_mix_logistic(x, args['nr_logistic_mix'])\n",
        "\n",
        "elif 'STL10' in args['dataset']:\n",
        "    train_loader = torch.utils.data.DataLoader(datasets.STL10(args['data_dir'], split='train', \n",
        "                                                              download=True, transform=ds_transforms), batch_size=args['batch_size'], shuffle=True, **kwargs)\n",
        "    test_loader  = torch.utils.data.DataLoader(datasets.STL10(args['data_dir'], split='test', \n",
        "                                                              transform=ds_transforms), batch_size=args['batch_size'], shuffle=True, **kwargs)\n",
        "    loss_op   = lambda real, fake : discretized_mix_logistic_loss(real, fake)\n",
        "    elbo_op = lambda mean, logvariance: elbo_loss(mean, logvariance)\n",
        "    sample_op = lambda x : sample_from_discretized_mix_logistic(x, args['nr_logistic_mix'])\n",
        "\n",
        "else :\n",
        "    raise Exception('{} dataset not in {STL10, cifar10}'.format(args['dataset']))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/pixel_CNN_data\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://app.wandb.ai/nerdk312/pytorch-PixelCNN\" target=\"_blank\">https://app.wandb.ai/nerdk312/pytorch-PixelCNN</a><br/>\n",
              "                Run page: <a href=\"https://app.wandb.ai/nerdk312/pytorch-PixelCNN/runs/d0ltx53q\" target=\"_blank\">https://app.wandb.ai/nerdk312/pytorch-PixelCNN/runs/d0ltx53q</a><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3XrdJe3vc5d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "torch.cuda.empty_cache() # Nawid - I am not sure if this helps - hopefully it does\n",
        "\n",
        "model = PixelCNN(nr_resnet=args['nr_resnet'], nr_filters=args['nr_filters'],\n",
        "            input_channels=input_channels,image_size = desired_size, nr_logistic_mix=args['nr_logistic_mix']) # Nawid - Instantiates the model\n",
        "if torch.cuda.device_count() > 1:\n",
        "  print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
        "  #model = nn.DataParallel(model,) # Nawid - Separate across different GPUS\n",
        "  model = nn.DataParallel(model, device_ids=[0, 1]) # Nawid - Specifying which GPUs to use to see if this makes a difference\n",
        "model.to(device)\n",
        "\n",
        "print('Model instantiated')\n",
        "\n",
        "if args['load_params']:\n",
        "    load_part_of_model(model, args['load_params'])\n",
        "    print('model parameters loaded')\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=args['lr'])\n",
        "scheduler = lr_scheduler.StepLR(optimizer, step_size=1, gamma=args['lr_decay'])\n",
        "\n",
        "torch.cuda.empty_cache() # Nawid - I am not sure if this helps - hopefully it does\n",
        "\n",
        "print('starting training')\n",
        "best_test_loss = 1e6 # Nawid - Set a very high initial maximum test loss so that the model will likely be higher than this\n",
        "\n",
        "\n",
        "for epoch in range(args['max_epochs']):\n",
        "    print('epoch_number',epoch)\n",
        "    model.train(True)\n",
        "    torch.cuda.synchronize()\n",
        "    train_loss = 0.\n",
        "    time_ = time.time()\n",
        "    model.train()\n",
        "\n",
        "    for batch_idx, (input,_) in enumerate(train_loader):\n",
        "        input = input.cuda(non_blocking=True) # Nawid- Changed async= True to non_blocking= True as async gets depreciated\n",
        "        #input = input.cuda(async=True)\n",
        "        \n",
        "        input = Variable(input)\n",
        "        output,mu,logvar = model(input)\n",
        "    \n",
        "        loss = loss_op(input, output) + elbo_op(mu,logvar)  # Nawid - loss_op represents the discretised mixture of logistics loss function\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.data.item()#[0]\n",
        "        \n",
        "        if (batch_idx +1) % args['print_every'] == 0: \n",
        "            deno = args['print_every'] * args['batch_size'] * np.prod(obs) * np.log(2.)\n",
        "            wandb.log({'train loss':(train_loss / deno)})\n",
        "            print('loss : {:.4f}, time : {:.4f}'.format(\n",
        "                (train_loss / deno), \n",
        "                (time.time() - time_)))\n",
        "            train_loss = 0.\n",
        "            time_ = time.time()\n",
        "    \n",
        "\n",
        "  # decrease learning rate\n",
        "    scheduler.step()\n",
        "    torch.cuda.empty_cache() # Nawid - This could possibly help with the lack of memory\n",
        "  \n",
        "    torch.cuda.synchronize()\n",
        "    model.eval()\n",
        "    test_loss = 0.\n",
        "    total_test_ll_loss = 0. # Nawid - Separate log likelihood loss\n",
        "    total_test_reg_loss = 0. # Nawid - Separate regularisation loss\n",
        "  \n",
        "    for batch_idx, (input,_) in enumerate(test_loader):\n",
        "        torch.cuda.empty_cache() # Nawid - This could possibly help with the lack of memory\n",
        "        #input = input.cuda(async=True)\n",
        "        input = input.cuda(non_blocking=True)\n",
        "        input_var = Variable(input) # Nawid - Should change to torch.no_grad !!!!\n",
        "        output, mu, logvar = model(input_var) \n",
        " \n",
        "    \n",
        "        test_ll_loss = loss_op(input_var, output) \n",
        "        test_reg_loss = elbo_op(mu,logvar)\n",
        "    \n",
        "        test_loss += test_ll_loss.item() + test_reg_loss.item()\n",
        "        total_test_ll_loss += test_ll_loss.item()\n",
        "        total_test_reg_loss += test_reg_loss.item()\n",
        "    \n",
        "        del test_ll_loss, test_reg_loss, output\n",
        "    \n",
        "\n",
        "    deno = batch_idx * args['batch_size'] * np.prod(obs) * np.log(2.)\n",
        "    wandb.log({'test loss':(test_loss / deno), 'test loglikelihood loss':(total_test_ll_loss / deno),'test reg loss':(total_test_reg_loss / deno)})\n",
        "    \n",
        "    print('test loss : {:.5f}, log likelihood loss : {:.5f}, regularisation loss {:.5f}'.format(\n",
        "        (test_loss/deno),\n",
        "        (total_test_ll_loss / deno),\n",
        "        (total_test_reg_loss / deno)))\n",
        "  \n",
        "    if (epoch + 1) % args['save_interval'] == 0: \n",
        "        torch.save(model.state_dict(), os.path.join(wandb.run.dir,'{}_{}.pth'.format(model_name, epoch))) # Nawid - Used to save the model in the wandb directory\n",
        "        wandb.save(os.path.join(wandb.run.dir,'{}_{}.pth'.format(model_name, epoch))) # Nawid -  Used to save in the wandb directory -  Not sure if it saves twice or once at the moment\n",
        "\n",
        "        '''\n",
        "        print('sampling...')\n",
        "        sample_t = sample(model)\n",
        "        sample_t = rescaling_inv(sample_t)\n",
        "        wandb.log({'sampled_image_{}'.format(epoch):wandb.Image(sample_t)})\n",
        "        '''\n",
        "        \n",
        "        reconstruct_t, original_t = reconstruct_image(model,test_loader)\n",
        "        reconstruct_t = rescaling_inv(reconstruct_t)\n",
        "        original_t = rescaling_inv(original_t)\n",
        "\n",
        "        wandb.log({'reconstructed_image_{}'.format(epoch):wandb.Image(reconstruct_t)})\n",
        "        wandb.log({'original_image_{}'.format(epoch):wandb.Image(original_t)})\n",
        "        #wandb.log({'sampled_image_{}'.format(epoch):wandb.Image(sample_t)})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5aOop3sQyNsR",
        "colab_type": "code",
        "outputId": "8c0f99f8-058c-4f6c-f634-7327bbc182b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "import shutil\n",
        "shutil.rmtree(os.path.join('/content/gdrive/My Drive/pixel_cnn_runs', model_name))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-c06c4b45d610>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshutil\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmtree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/gdrive/My Drive/pixel_cnn_runs'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/lib/python3.6/shutil.py\u001b[0m in \u001b[0;36mrmtree\u001b[0;34m(path, ignore_errors, onerror)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0morig_st\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0monerror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/shutil.py\u001b[0m in \u001b[0;36mrmtree\u001b[0;34m(path, ignore_errors, onerror)\u001b[0m\n\u001b[1;32m    473\u001b[0m         \u001b[0;31m# lstat()/open()/fstat() trick.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m             \u001b[0morig_st\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0monerror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/gdrive/My Drive/pixel_cnn_runs/pcnn_lr:0.00020_nr-resnet3_nr-filters60'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2B7_VxutmNri",
        "colab_type": "text"
      },
      "source": [
        "# Practice code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NX5b4Ju2ffHy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = PixelCNN(nr_resnet=args['nr_resnet'], nr_filters=args['nr_filters'], \n",
        "            input_channels=input_channels, nr_logistic_mix=args['nr_logistic_mix']) # Nawid - Instantiates the mode\n",
        "\n",
        "\n",
        "reconstruct_t, original_t = reconstruct_image(model,test_loader)\n",
        "reconstruct_t = rescaling_inv(reconstruct_t)\n",
        "reconstruct_t = reconstruct_t.permute(0,2,3,1).detach().cpu().numpy()\n",
        "\n",
        "original_t = rescaling_inv(original_t)\n",
        "original_t = original_t.permute(0,2,3,1).detach().cpu().numpy()\n",
        "single_reconstruct_t = reconstruct_t[0]\n",
        "single_original_t = original_t[0]\n",
        "\n",
        "Image.fromarray((single_reconstruct_t * 255).astype('uint8')).save(os.path.join(images_models,'{}_{}_{}.png'.format(model_name, 1,'reconstruction')))\n",
        "Image.fromarray((single_original_t * 255).astype('uint8')).save(os.path.join(images_models,'{}_{}_{}.png'.format(model_name, 1,'original')))\n",
        "'''\n",
        "single_reconstruct_t = reconstruct_t[0]\n",
        "single_original_t = original_t[0]\n",
        "utils.save_image(single_reconstruct_t,os.path.join(images_models,'{}_{}_{}.png'.format(model_name, 1,'reconstruction')),nrow=1)\n",
        "utils.save_image(single_original_t,os.path.join(images_models,'{}_{}_{}.png'.format(model_name, 1,'original')),nrow=1)\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjf-00zgc6Ua",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "#torch.save(model.state_dict(), \"model.h5\")\n",
        "#wandb.save('model.h5')\n",
        "\n",
        "#torch.save(model.state_dict(),'{}_{}.pth'.format(model_name, epoch))\n",
        "#wandb.save('{}_{}.pth'.format(model_name, epoch))\n",
        "\n",
        "#wandb.save()\n",
        "#torch.save(model.state_dict(), os.path.join(wandb.run.dir,'{}_{}.pth'.format(model_name, epoch))) # Nawid - Used to save the model in the wandb directory\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}